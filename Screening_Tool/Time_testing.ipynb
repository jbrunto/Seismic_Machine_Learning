{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3568e433",
   "metadata": {},
   "source": [
    "# Time tests for custom-built methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc559d41",
   "metadata": {},
   "source": [
    "This Notebook explores the processing cost of various functions that are key in the Fault Leakage Screening notebook. A few methods do not play a signficant role in the computational time in this study, and therefore those have not been tested for time. The standard for measurement seen throughout this notebook is how long the method would take to run with 2.44 GB volumes in the form of numpy arrays. 2.44 GB was chosen specifically, because this is the size of the segy volumes used in this study when converted to numpy arrays. \n",
    "\n",
    "This is a very elementary and introductory view into the computational costs of these methods, as the times displayed are approximated, and not averaged over a set of runs of the same method. The motivation of this notebook was to give an initial insight as to how this methods might perform when applied to different datasets, should the work of this study be used elsewhere in the future. These methods were run locally on a Mac Os-X CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db34fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seismic_toolkit import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70cffdf",
   "metadata": {},
   "source": [
    "### 0. Importing the data for time tests.\n",
    "For this exercise we need a set of unpatched data, a set of patched coherency data, a set of patched FWI velocity data, and an array of masks that match the shape of the FWI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0dfabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpatched_data = load_segy('./Samson_cropped.segy', verbose=False)\n",
    "patched_coherency = np.load('./marfurt_100.npy')\n",
    "FWI = np.load('./FWI_patched_train.npy')\n",
    "masks = np.load('./train_masks.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbabde8b",
   "metadata": {},
   "source": [
    "### 1. patch( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d1053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent of data used for time test\n",
    "start = time.time()\n",
    "percent = 100/unpatched_data.shape[2]\n",
    "dummy = patch(unpatched_data[:, :, 0:100])\n",
    "execution_time = time.time() - start\n",
    "del dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "838d79bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate execution time to run this function on 2.44 GB data: \n",
      " 4.883 Minutes\n"
     ]
    }
   ],
   "source": [
    "print('Approximate execution time to run this function on 2.44 GB data: \\n',\n",
    "      round(((execution_time / percent) / 60), 3), 'Minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff10d47",
   "metadata": {},
   "source": [
    "### 2. unpatch( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d67d0195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate execution time to run this function on 2.44 GB data: \n",
      " 17.892 Seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dummy = unpatch(patched_coherency, image_size=[611, 600, 1500])\n",
    "execution_time = time.time() - start\n",
    "del dummy\n",
    "print('Approximate execution time to run this function on 2.44 GB data: \\n',\n",
    "      round((execution_time), 3), 'Seconds')                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3820cd",
   "metadata": {},
   "source": [
    "### 3. cluster( ) - a single run of the K-Mean algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fc6b29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate execution time to run this function for one image: \n",
      " 0.06836199760437012 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dummy = cluster(patched_coherency[0, :, :].reshape((10000, 1)))\n",
    "execution_time = time.time() - start\n",
    "del dummy\n",
    "print('Approximate execution time to run this function for one image: \\n',\n",
    "      execution_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53c9d77",
   "metadata": {},
   "source": [
    "### 4. score_inertia( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99ae9691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate execution time to run this function on 2.44 GB data: \n",
      " 3144.866 Minutes\n"
     ]
    }
   ],
   "source": [
    "marfurt_subset = patched_coherency[0:100, :, :].reshape((100, 10000, 1))\n",
    "# percent of data\n",
    "percent = 100 / patched_coherency.shape[0]\n",
    "start = time.time()\n",
    "dummy = score_inertia(marfurt_subset)\n",
    "execution_time = time.time() - start\n",
    "del dummy\n",
    "print('Approximate execution time to run this function on 2.44 GB data: \\n',\n",
    "      round(((execution_time / percent) / 60), 3), 'Minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3bf0c2",
   "metadata": {},
   "source": [
    "### 5. score_var( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f71b49c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate execution time to run this function on 2.44 GB data: \n",
      " 240.725 Hours\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dummy = score_var(marfurt_subset)\n",
    "execution_time = time.time() - start\n",
    "del dummy\n",
    "print('Approximate execution time to run this function on 2.44 GB data: \\n',\n",
    "      round(((execution_time / percent) / 3600), 3), 'Hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963903a0",
   "metadata": {},
   "source": [
    "### 6. predict( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "032554f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate execution time to run this function on 2.44 GB data: \n",
      " 34.543 Minutes\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dummy = predict(marfurt_subset, verbose=False)\n",
    "execution_time = time.time() - start\n",
    "del dummy\n",
    "print('Approximate execution time to run this function on 2.44 GB data: \\n',\n",
    "      round(((execution_time / percent) / 60), 3), 'Minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede1ece",
   "metadata": {},
   "source": [
    "### 7. mask( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84070eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate execution time to run this function on all data: \n",
      " 11.905 Seconds\n"
     ]
    }
   ],
   "source": [
    "percent = 0.2 # test set size\n",
    "start = time.time()\n",
    "dummy = mask(FWI, masks)\n",
    "execution_time = time.time() - start\n",
    "del dummy\n",
    "print('Approximate execution time to run this function on all data: \\n',\n",
    "      round(((execution_time / percent)), 3), 'Seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b3e69",
   "metadata": {},
   "source": [
    "### 8. marfurt( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0192700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonbrunton/irp-jab121/seismic_toolkit.py:425: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sembl = square_of_sums.sum() / sum_of_squares.sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate execution time to run this function on all data: \n",
      " 2.098 Hours\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "percent = 30/unpatched_data.shape[2]\n",
    "dummy = marfurt(unpatched_data[:, :, 0:30])\n",
    "execution_time = time.time() - start\n",
    "del dummy\n",
    "print('Approximate execution time to run this function on all data: \\n',\n",
    "      round(((execution_time / percent)/3600), 3), 'Hours')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b54251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
